{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pprint\n",
    "import en_coref_md\n",
    "from nltk import Tree\n",
    "from ipynb.fs.full.stanford_open_ie_python_wrapper import stanford_ie\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = \"\"\"\n",
    "    NP:    {<DT><WP><VBP>*<RB>*<VBN><IN><NN>}\n",
    "           {<NN|NNS|NNP|NNPS><IN>*<NN|NNS|NNP|NNPS>+}\n",
    "           {<JJ|JJR|JJS>*<NN|NNS|NNP|NNPS><CC>*<NN|NNS|NNP|NNPS>+}\n",
    "           {<JJ|JJR|JJS>*<NN|NNS|NNP|NNPS>+}\n",
    "           \n",
    "    \"\"\"\n",
    "\n",
    "NPChunker = nltk.RegexpParser(patterns)\n",
    "coref_parser = en_coref_md.load()\n",
    "stanford_corenlp_path = \"/Users/krishna.aruru/stanfordnlp_resources/stanford-corenlp-full-2018-10-05\"\n",
    "os.environ[\"CORENLP_HOME\"] = \"/Users/krishna.aruru/stanfordnlp_resources/stanford-corenlp-full-2018-10-05\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tree(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    sentences = [NPChunker.parse(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "def get_noun_phrases(text):\n",
    "    sentences = convert_to_tree(text)\n",
    "    nps = []\n",
    "    for sent in sentences:\n",
    "        tree = NPChunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == \"NP\":\n",
    "                nps.append(\" \".join([word for word, _ in subtree.leaves()]))\n",
    "    return nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corefs(paragraph):\n",
    "    doc = coref_parser(paragraph)\n",
    "    refs = {}\n",
    "    if doc._.has_coref:\n",
    "        for cluster in doc._.coref_clusters:\n",
    "            for mention in cluster.mentions:\n",
    "                refs[mention.start_char] = ( mention.end_char, cluster.main.text)\n",
    "    return refs\n",
    "\n",
    "def deref_text(sentence, coref_mapping):\n",
    "    output = \"\"\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        if i in coref_mapping:\n",
    "            pos, replacement = coref_mapping[i]\n",
    "            output += replacement\n",
    "            if pos == i:\n",
    "                i += 1\n",
    "            else:\n",
    "                i = pos\n",
    "        else:\n",
    "            output += sentence[i]\n",
    "            i += 1\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    output = []\n",
    "    for word in nltk.tokenize.word_tokenize(text):\n",
    "        if not word.lower() in stopwords:\n",
    "            output.append(word)\n",
    "    removed = \" \".join(output)\n",
    "    if not removed:\n",
    "        return text\n",
    "    return removed\n",
    "\n",
    "def get_ngrams(sentence):\n",
    "    print(\"Getting n grams for: {}\".format(sentence))\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    req_n_grams = list()\n",
    "    if len(tokens) <= 4:\n",
    "        return [sentence]\n",
    "    for i in range(4, len(tokens) + 1):\n",
    "        n_grams = nltk.ngrams(tokens, i)\n",
    "        for gram in n_grams:\n",
    "            req_n_grams.append(\" \".join(gram))\n",
    "    return req_n_grams\n",
    "\n",
    "def preprocess(paragraph, generate_ngrams=False):\n",
    "    ref_mapping = get_corefs(paragraph)\n",
    "    paragraph = deref_text(paragraph, ref_mapping)\n",
    "    # \";\" is used to seperate the subject, relation and object by both Stanford and OpenIE. \n",
    "    # Better to remove it from out text so that we don't get confused in output.\n",
    "    paragraph.replace(\";\", \"\")\n",
    "    paragraph = [line for line in nltk.sent_tokenize(paragraph)\n",
    "                     if len(nltk.word_tokenize(line)) > 3]\n",
    "    if generate_ngrams:\n",
    "        paragraph = flatten([get_ngrams(line) for line in paragraph])\n",
    "    return paragraph\n",
    "\n",
    "def flatten(list_2d):\n",
    "    return [item for sublist in list_2d for item in sublist]\n",
    "\n",
    "def process_batch(batch, generate_ngrams=False):\n",
    "    batch = [preprocess(line, generate_ngrams) for line in batch]\n",
    "    batch = flatten(batch)\n",
    "    nps = [get_noun_phrases(line) for line in batch]\n",
    "    nps = set(flatten(nps))\n",
    "    return batch, nps\n",
    "\n",
    "def filter_relations(relations, nounphrases):\n",
    "    rels = [(subj, relation, obj) for subj, relation, obj in relations if (subj in nounphrases or obj in nounphrases)]\n",
    "    rels = [(remove_stopwords(subj), remove_stopwords(relation), remove_stopwords(obj)) \n",
    "            for subj, relation, obj in rels]\n",
    "    return rels\n",
    "\n",
    "def get_relations(paragraph):\n",
    "    paragraph = preprocess(paragraph)\n",
    "    nps = set(flatten([get_noun_phrases(line) for line in paragraph]))\n",
    "    rels = stanford_ie(paragraph)\n",
    "    return filter_relations(rels, nps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [line.strip() for line in open(\"vogue_non_empty_descriptions.txt\").readlines()]\n",
    "# sentences, nps = process_batch(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "# for i in range(0, len(sentences), 100):\n",
    "#     print(\"Processing batch: {}\".format(i/100))\n",
    "#     rels = stanford_ie(sentences[i: i+100])\n",
    "#     results.extend(filter_relations(rels, nps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"stanfordoie_outputs_without_ngrams.txt\", \"w\") as fw:\n",
    "#     for relation in results:\n",
    "#         fw.write(\"|\".join([x for x in relation]))\n",
    "#         fw.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: (7, 'Bananas'), 44: (48, 'Bananas')}\n",
      "['Bananas are excellent sources of potassium.', 'Bananas are also very tasty']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Bananas are excellent sources of potassium. They are also very tasty\"\n",
    "print(preprocess(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bananas are excellent sources of potassium.', 'Bananas are also very tasty']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
