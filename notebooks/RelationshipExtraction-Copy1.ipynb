{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LC_ALL=en_US.UTF-8\n",
      "env: LANG=en_US.UTF-8\n"
     ]
    }
   ],
   "source": [
    "%set_env LC_ALL=en_US.UTF-8\n",
    "%set_env LANG=en_US.UTF-8\n",
    "import os\n",
    "os.environ[\"LC_ALL\"] = \"en_US.UTF-8\"\n",
    "os.environ[\"LANG\"] = \"en_US.UTF-8\"\n",
    "import nltk\n",
    "# import MySQLdb\n",
    "import en_coref_md\n",
    "import sys\n",
    "\n",
    "nlp = en_coref_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_splitter(text):\n",
    "    lines_in_article = [s.strip() for s in text.splitlines()]\n",
    "    return lines_in_article\n",
    "\n",
    "def print_lines(lines):\n",
    "    for elem in lines:\n",
    "        print elem\n",
    "        print \"------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from nltk import ngrams\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "fw = open(\"tmpout\", \"wb\")\n",
    "fr = open(\"tmpout\", \"r\")\n",
    "\n",
    "proc = subprocess.Popen(['java', \"-mx8g\", \"-cp\", \"/Users/kumar.jitender/Downloads/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar:/Users/kumar.jitender/Downloads/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-models.jar:/Users/kumar.jitender/Downloads/stanford-corenlp-full-2017-06-09/CoreNLP-to-HTML.xsl:slf4j-api.jar:/Users/kumar.jitender/Downloads/stanford-corenlp-full-2017-06-09/slf4j-simple.jar\", \"edu.stanford.nlp.naturalli.OpenIE\",  \"-threads\",  \"8\", \"-ignore_affinity\", \"true\",  \"-ssplit.newlineIsSentenceBreak\", \"always\", \"-format\", \"ollie\"], stdout=fw, stderr=fw, stdin=subprocess.PIPE)\n",
    "\n",
    "proc.stdin.write(\"Milo Blah Blah\\n\")\n",
    "proc.stdin.flush()\n",
    "\n",
    "# Waiting for the model to load properly\n",
    "time.sleep(20)\n",
    "# Discarding initial Output\n",
    "print fr.read()\n",
    "\n",
    "def parse_oie_op(line):\n",
    "    if line.find(':') !=-1:\n",
    "        rel_substring = line[line.find(':') + 3:]\n",
    "        if rel_substring.find(';') != -1:\n",
    "            e1 = rel_substring[:rel_substring.find(';')]\n",
    "            rel_substring = rel_substring[rel_substring.find(';')+2:]\n",
    "            if rel_substring.find(';') != -1:\n",
    "                rel = rel_substring[:rel_substring.find(';')]\n",
    "                rel_substring = rel_substring[rel_substring.find(';')+2:]            \n",
    "                if rel_substring.find(')') != -1:\n",
    "                    e2 = rel_substring[:rel_substring.find(')')]\n",
    "                    return (e1, rel, e2)\n",
    "                else:\n",
    "                    raise ValueError(line)\n",
    "            else:\n",
    "                raise ValueError(line)\n",
    "        else:\n",
    "            raise ValueError(line)\n",
    "        \n",
    "\n",
    "def extract_relations(sentence):\n",
    "    no_of_tokens = len(nltk.word_tokenize(sentence))\n",
    "    myset=set()\n",
    "    for iter in range(max(4, no_of_tokens-5), no_of_tokens+1):\n",
    "        sixgrams = ngrams(sentence.split(), iter)\n",
    "        new_list = [' '.join(words) for words in sixgrams] \n",
    "        for gram in new_list:\n",
    "            proc.stdin.write( gram + \"\\n\")\n",
    "            proc.stdin.flush()\n",
    "        \n",
    "    time.sleep(10)\n",
    "    for line in fr.read().splitlines():\n",
    "        if 'No extractions in' not in line:\n",
    "            myset.add(parse_oie_op(line))\n",
    "    return myset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pprint\n",
    "from nltk import Tree\n",
    "\n",
    "# patterns = \"\"\"\n",
    "#     NP: {<JJ>*<NN*>+}\n",
    "#     {<JJ>*<NN*><CC>*<NN*>+}\n",
    "#     \"\"\"\n",
    "\n",
    "patterns = \"\"\"\n",
    "    NP:    {<DT><WP><VBP>*<RB>*<VBN><IN><NN>}\n",
    "           {<NN|NNS|NNP|NNPS><IN>*<NN|NNS|NNP|NNPS>+}\n",
    "           {<JJ|JJR|JJS>*<NN|NNS|NNP|NNPS><CC>*<NN|NNS|NNP|NNPS>+}\n",
    "           {<JJ|JJR|JJS>*<NN|NNS|NNP|NNPS>+}\n",
    "           \n",
    "    \"\"\"\n",
    "\n",
    "NPChunker = nltk.RegexpParser(patterns)\n",
    "\n",
    "def prepare_text(input):\n",
    "    sentences = nltk.sent_tokenize(input)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    sentences = [NPChunker.parse(sent) for sent in sentences]\n",
    "    print(sentences)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def parsed_text_to_NP(sentences):\n",
    "    nps = []\n",
    "    for sent in sentences:\n",
    "        tree = NPChunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NP':\n",
    "                t = subtree\n",
    "                t = ' '.join(word for word, tag in t.leaves())\n",
    "                nps.append(t)\n",
    "    return nps\n",
    "\n",
    "\n",
    "def find_nps(text):\n",
    "    prepared = prepare_text(text)\n",
    "    return parsed_text_to_NP(prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortSecond(val):\n",
    "    return val[1] \n",
    "\n",
    "def get_coreferences(paragraph):\n",
    "    doc = nlp(unicode(paragraph , 'latin-1'))\n",
    "    lst_replace = []\n",
    "    if doc._.has_coref:\n",
    "        for cluster in doc._.coref_clusters:\n",
    "            for mention in cluster.mentions:\n",
    "                lst_replace.append((mention.start_char, mention.end_char, mention.text, cluster.main.text))\n",
    "\n",
    "    lst_replace.sort(key=sortSecond)        \n",
    "    return lst_replace\n",
    "\n",
    "\n",
    "def get_coref_replacement_dict(coref_list, sentence_start_char, sentence_end_char):\n",
    "    relevant_corefs = {}\n",
    "    for coref in coref_list:\n",
    "        if coref[0] >= sentence_start_char and coref[1] <= sentence_end_char:\n",
    "            relevant_corefs[remove_stopwords(coref[2])] = coref[3]\n",
    "    return relevant_corefs\n",
    "\n",
    "\n",
    "def prcoess_relations(paragraph):\n",
    "\n",
    "    #     get all coreferences in paragraph\n",
    "    coref_list = get_coreferences(paragraph);\n",
    "#     print \"PARAGRAPH\"\n",
    "#     print paragraph\n",
    "#     print \"COREFERENCES\"\n",
    "#     print coref_list\n",
    "    \n",
    "    #     Split sentences here\n",
    "    all_sentences = nltk.sent_tokenize(paragraph)\n",
    "    \n",
    "    sentence_start_char = 0\n",
    "    sentence_end_char = 0\n",
    "    for sentence in all_sentences:\n",
    "        print (\"-----------------------------\")\n",
    "        print (\"SENTENCE\")\n",
    "        print (sentence)\n",
    "\n",
    "        sentence_start_char = sentence_end_char\n",
    "        sentence_end_char = sentence_start_char + len(sentence)\n",
    "        \n",
    "        #     Run Relationship extraction on sentence via Stanford NLP\n",
    "        relationships_set = extract_relations(sentence)\n",
    "        print (\"ALL RELATIONSHIPS\")\n",
    "        print (relationships_set)\n",
    "        \n",
    "        #     Figure stopword removed coreference replacements for that sentence\n",
    "        coref_replacement_dict = get_coref_replacement_dict(coref_list, sentence_start_char, sentence_end_char)\n",
    "        print (\"COREF REPLACEMENTS\")\n",
    "        print (coref_replacement_dict)\n",
    "\n",
    "        #     Remove stopwords from relations\n",
    "        relationships_set_stopword_remove = set()\n",
    "        for relation in relationships_set:\n",
    "            if not remove_stopwords(relation[1]):\n",
    "                new_relation = (remove_stopwords(relation[0]), relation[1], remove_stopwords(relation[2]))\n",
    "            else:\n",
    "                new_relation = (remove_stopwords(relation[0]), remove_stopwords(relation[1]), remove_stopwords(relation[2]))\n",
    "            relationships_set_stopword_remove.add(new_relation)\n",
    "\n",
    "        print (\"RELATIONSHIPS AFTER REMOVING STOPWORDS\")\n",
    "        print (relationships_set_stopword_remove)\n",
    "        \n",
    "            \n",
    "        #     Replace coreferences\n",
    "        relationships_set_coref_replace = set()\n",
    "        for relation in relationships_set_stopword_remove:\n",
    "            if relation[0] in coref_replacement_dict:\n",
    "                e1 = coref_replacement_dict[relation[0]]\n",
    "            else:\n",
    "                e1 = relation[0]\n",
    "            if relation[2] in coref_replacement_dict:\n",
    "                e2 = coref_replacement_dict[relation[2]]\n",
    "            else:\n",
    "                e2 = relation[2]\n",
    "            new_relation = (e1, relation[1], e2)\n",
    "            relationships_set_coref_replace.add(new_relation)\n",
    "        \n",
    "        print (\"RELATIONSHIPS AFTER CoREF REPLACEMENT\")\n",
    "        print (relationships_set_coref_replace)\n",
    "        \n",
    "        \n",
    "        #     Figure Nounphrases for that sentence                \n",
    "        noun_phrases = find_nps(sentence)\n",
    "        print (\"NOUN PHRASES\")\n",
    "        print (noun_phrases)\n",
    "        \n",
    "        # Replace corefs in noun_phrases as well\n",
    "        # Noun phrases are already stopword removed, if not, remove\n",
    "        \n",
    "        noun_phrases_coref_filtered = set()\n",
    "        for noun_phrase in noun_phrases:\n",
    "            noun_phrase_temp = remove_stopwords(noun_phrase)\n",
    "            if noun_phrase_temp in coref_replacement_dict:\n",
    "                noun_phrases_coref_filtered.add(coref_replacement_dict[noun_phrase_temp])\n",
    "            else:\n",
    "                noun_phrases_coref_filtered.add(noun_phrase_temp)\n",
    "                            \n",
    "        filtered_relationships_set = set()\n",
    "        #     Filter relations on the NPhrases\n",
    "        for relation in relationships_set_coref_replace:\n",
    "            if relation[0] in noun_phrases_coref_filtered and relation[2] in noun_phrases_coref_filtered:\n",
    "                filtered_relationships_set.add(relation)\n",
    "                \n",
    "        print (\"RELATIONSHIPS AFTER NOUN PHRASE FIltering\")\n",
    "        print (filtered_relationships_set)\n",
    "    return filtered_relationships_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_nps(\"Celebrity-favourite label Pumas shoes already come with a firm fan following but throw in Sophia Websters signature, playful aesthetic and youre guaranteed cult status.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    filtered_sentence = []\n",
    "    \n",
    "    for w in word_tokens:\n",
    "        if w.lower() not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "        \n",
    "    return ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = MySQLdb.connect(host=\"localhost\",    # your host, usually localhost\n",
    "                     user=\"root\",         # your username\n",
    "                     passwd=\"foobar\",  # your password\n",
    "                     db=\"new_trends\")        # name of the data base\n",
    "\n",
    "cur = db.cursor()\n",
    "\n",
    "# Use all the SQL you like\n",
    "cur.execute(\"SELECT * FROM TEXTS WHERE SOURCE = 'DIFFBOT' and type = 'BODY' and id =93\")\n",
    "\n",
    "# print all the first cell of all the rows\n",
    "for row in cur.fetchall():\n",
    "    if row[1]:\n",
    "        lines = line_splitter(row[1])\n",
    "        for line in lines:\n",
    "            prcoess_relations(line)\n",
    "        print \"********************************************************************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "After Kate Middleton and Meghan Markle stepped out together in navy dresses, Diana Penty made an appearance in a deep blue kurta set day before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
